---
title: "Chapter 17 Big picture (metaprogramming)"
author: "Yang Hu"
format: 
    html:
      self-contained: true
      embed-resources: true

number-sections: false
code-annotations: hover
    
toc: true
toc-title: "Contents"
toc-location: "left"
toc-depth: 4

execute: 
  freeze: auto
  warning: true
---

One of the most intriguing things about R is its ability to do **metaprogramming**. This is the idea that **code is data that can be inspected and modified programmatically. This is a powerful idea**; one that deeply influences much R code. 

At the most basic level, it allows you to do things like write `library(purrr)` instead of `library("purrr")` and enable `plot(x, sin(x))` to automatically label the axes with `x` and `sin(x)`. 

At a deeper level, it allows you to do things like use `y ~ x1 + x2` to represent a model that predicts the value of y from x1 and x2, to translate `subset(df, x == y)` into `df[df$x == df$y, , drop = FALSE]`, and to use `dplyr::filter(db, is.na(x))` to generate the SQL `WHERE x IS NULL` when `db` is a remote database table.

Closely related to metaprogramming is **non-standard evaluation**, NSE for short. This term, which is commonly used to describe the behaviour of R functions, is problematic in two ways. Firstly, NSE is actually a property of the argument (or arguments) of a function, so talking about NSE functions is a little sloppy. Secondly, it’s confusing to define something by what it’s not (standard), so in this book I’ll introduce more precise vocabulary.

Specifically, this book focuses on tidy evaluation (sometimes called tidy eval for short). Tidy evaluation is implemented in the rlang package, and I’ll use rlang extensively in these chapters. This will allow you to focus on the big ideas, without being distracted by the quirks of implementation that arise from R’s history. After I introduce each big idea with rlang, I’ll then circle back to talk about how those ideas are expressed in base R. This approach may seem backward to some, but it’s like learning how to drive using an automatic transmission rather than a stick shift: it allows you to focus on the big picture before having to learn the details. This book focusses on the theoretical side of tidy evaluation, so you can fully understand how it works from the ground up. If you are looking for a more practical introduction, I recommend the tidy evaluation book at https://tidyeval.tidyverse.org.


```{r}
library(rlang)
library(lobstr)

library(stringr)
library(purrr)
```

```{r}
# Basic version with condition:
x <- ifelse(runif(n = 1) <= 0.5, 0, 100)

expr(sum(x, 2, 3)) |> eval()


# List version:
parameters <- list(n = 3, min = 0, max = 1)

x <- runif(parameters)

expr(sum(x)) |> eval()


# Purrr version:
map_dbl(1:100, \(.x){
  x <- ifelse(runif(n = 1) <= 0.5, 0, 100)

expr(sum(x, 2, 3)) |> eval()
})
```


## Introduction

Metaprogramming is the hardest topic in this book because it brings together many formerly unrelated topics and forces you grapple with issues that you probably haven’t thought about before. You’ll also need to learn a lot of new vocabulary, and at first it will seem like every new term is defined by three other terms that you haven’t heard of. Even if you’re an experienced programmer in another language, your existing skills are unlikely to be much help as few modern popular languages expose the level of metaprogramming that R provides. So don’t be surprised if you’re frustrated or confused at first; this is a natural part of the process that happens to everyone!

But I think it’s easier to learn metaprogramming now than ever before. Over the last few years, the theory and practice have matured substantially, providing a strong foundation paired with tools that allow you to solve common problems. In this chapter, you’ll get the big picture of all the main pieces and how they fit together.


## Code is data

The first big idea is that **code is data**: **you can capture code and compute on it as you can with any other type of data**.

The first way you can capture code is with `rlang::expr()`. You can think of `expr()` as returning exactly what you pass in:

```{r}
expr(updatefi::get_econ_fi())

expr(mean(x, na,rm = TRUE))

expr(10 + 100 + 1000) |> typeof()

# Try
map(1:5, \(.x) {.x + .x})                        # Calculate as they are

map(1:5, \(.x) {expr(.x + .x)})                  # Shown as `.x + .x`, not calculated

map(1:5, \(.x) {expr(!!.x + .x)})                # Shown as `1 + .x`, not calculated

map(1:5, \(.x) {expr(!!.x + !!.x)})              # Shown as `1 + 1`, not calculated

map(1:5, \(.x) {expr(!!.x + !!.x) |> eval()})    # Shown as `1 + 1`, calculated


# Based on these, modify the arguments based on conditions:

map_dbl(1:5, \(.x) {
  
  if (.x %% 2 == 1) {   # If .x is an odd number
    
    .x <- .x + 1
    
    expr(!!.x + !!.x) |> eval()
    
  }
  
  expr(!!.x + !!.x) |> eval()})    # Shown as `1 + 1`, calculated

```

More formally, captured code is called an expression. An expression isn’t a single type of object, but is a collective term for any of four types (call, symbol, constant, or pairlist), which you’ll learn more about in Chapter 18.

`expr()` lets you capture code that you’ve typed. You need a different tool to capture code passed to a function because `expr()` doesn’t work:

```{r}
capture_it <- function(x) {expr(x)}

capture_it(dasdsa)
```

Here you need to use a function specifically designed to capture user input in a function argument: `enexpr()`. Think of the “en” in the context of “enrich”: `enexpr()` takes a lazily evaluated argument and turns it into an expression:

```{r}
capture_it <- function(x) {enexpr(x)}

capture_it(1 + 2 + 3)

# enexpr() is designed to capture expressions in a way that reflects their evaluation in the context of a function call. 
# It looks at the internal promise object that powers lazy evaluation in R, allowing it to capture the actual expressions passed to a function, even if those expressions involve variables whose values are determined later. This makes enexpr() particularly useful in function definitions where you want to work with the expressions passed as arguments, rather than their evaluated results.
```

Because `capture_it()` uses `enexpr()` we say that it automatically quotes its first argument. You’ll learn more about this term in Section 19.2.1.

**Once you have captured an expression, you can inspect and modify it**. Complex expressions behave much like lists. That means you can modify them using `[[` and `$`:


```{r}
# Add an inappropriate argument from ggplot2:
f <- expr(mean(x = c(1, 2, 3, NA), na.rm = TRUE, mapping = aes(x = x, y = y)))

# Try to evaluate it:
eval(f)

# Remove the inappropriate argument:
f$mapping <- NULL

eval(f)

# Change the valid argument to FALSE:
f$na.rm <- FALSE

eval(f)

# Change the valid argument to exclude NA:
f$x <- c(1, 2, 3)

eval(f)


# new:
(f <- expr(list(x = 1, y = 2)))


# Add a new argument:
f$z <- 3

f

# or remove an argument:

f$y <- NULL

f

```

The first element of the call is the function to be called, which means the first argument is in the second position. You’ll learn the full details in Section 18.3.

```{r}
purrr::map(1:3, \(.x) {
  f[[.x]]
})


# Find each argument by name:
f[["x"]]

f[["na.rm"]]


# Find each argument by position:
f[[1]]

f[[2]]
```


## Code is a tree

To do more complex manipulation with expressions, you need to fully understand their structure. **Behind the scenes, almost every programming language represents code as a tree**, often called the **abstract syntax tree**, or AST for short. R is unusual in that you can actually inspect and manipulate this tree.

A very convenient tool for understanding the tree-like structure is `lobstr::ast()`. Given some code, this function displays the underlying tree structure. Function calls form the branches of the tree, and are shown by rectangles. The leaves of the tree are symbols (like `a`) and constants (like "`b`").

```{r}
ast(f(a, "b"))

ast(mean(x = c(1, 2, 3, NA), na.rm = TRUE))

ast(mean(x = c(1, 2, 3, NA), na.rm = TRUE))


str_detect(string = c("Yang", "Lara", "Eric"), pattern = "c$", negate = FALSE) |> ast()
```


Nested function calls create more deeply branching trees:

```{r}
ast(
  sum(
    runif(n   = 5, 
          min = 0, 
          max = 1),
    c(1, 2, 3)
  )
)
```

Because all function forms can be written in prefix form (Section 6.8.2), every R expression can be displayed in this way:

```{r}
# It also shows the order of operations:
ast(1 + 2 * -3)
```

Displaying the AST in this way is a useful tool for exploring R’s grammar, the topic of Section 18.4.


## Code can generate code (!)

As well as seeing the tree from code typed by a human, you can also use code to create new trees. There are two main `tools: call2()` and unquoting.


`rlang::call2()` constructs a function call from its components: the function to call, and the arguments to call it with.

```{r}
call2("f", 1, 2, 3)

call2("+", 1, call2("*", 2, 3))

# new: 

call2("mean", call2("c", 1, 2, 3))
```

`call2()` is often convenient to program with, but is a bit clunky for interactive use. An alternative technique is to build complex code trees by combining simpler code trees with a template. `expr()` and `enexpr()` have built-in support for this idea via `!!` (pronounced bang-bang), the **unquote operator**. 反引用操作符

The precise details are the topic of Section 19.4, but basically **`!!x` inserts the code tree stored in `x` into the expression**. This makes it easy to build complex trees from simple fragments:

```{r}
xx <- expr(x + x)

yy <- expr(y + y)

expr(!!xx / !!yy)


# new attempt for a simple map function:

fn <- expr(\(.x) {.x + .x})   # Inner function of map()

expr(map_dbl(1:3, !!fn))      # Assemble it with the main map() function

# or call2("map_dbl", 1:3, fn)

# Test:
expr(map_dbl(1:3, !!fn)) |> eval() 
```

Notice that the output preserves the operator precedence so we get `(x + x) / (y + y)` not `x + x / y + y` (i.e. `x + (x / y) + y`). This is important, particularly if you’ve been wondering if it wouldn’t be easier to just paste strings together.

Unquoting gets even more useful when you wrap it up into a function, first using `enexpr()` to capture the user’s expression, then `expr()` and `!!` to create a new expression using a template. The example below shows how you can generate an expression that computes the coefficient of variation:

```{r}
cv <- function(var) {
  
  # This step preserves the user input as it is:
  var <- enexpr(var)
  
  # This step transform the input into R code with the help of !!:
  expr(sd(!!var) / mean(!!var))
}

cv(12 + 2)
```

(This isn’t very useful here, but being able to create this sort of building block is very useful when solving more complex problems.)

Importantly, this works even when given weird variable names:

```{r}
cv(`~`)
```

Dealing with weird names is another good reason to avoid `paste()` when generating R code. You might think this is an esoteric concern, but not worrying about it when generating SQL code in web applications led to [**SQL injection attacks**]{style="color:red;"} that have collectively cost billions of dollars.


```{r}
library(tidyverse)


# default:
iris |> 
  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point()


# Modified:
iris |> 
  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point(aes(colour = Species)) +
  theme_bw()


# My attempt:
my_modi <- function(x) {
  var <- enexpr(x)
  
  expr(iris |> 
  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) + 
           geom_point(aes(colour = !!var)) + theme_bw()
  
  )
}

# Insert the parameters:
my_modi(Species) |> eval()
```


## Evaluation runs code

Inspecting and modifying code gives you one set of powerful tools. You get another set of powerful tools when you **evaluate**, i.e. execute or run, an expression. Evaluating an expression requires an environment, which tells R what the symbols in the expression mean. You’ll learn the details of evaluation in Chapter 20.

The primary tool for evaluating expressions is `base::eval()`, which takes an expression and an environment:

```{r}
x <- 10
y <- 10

# Local environment:
eval(expr = expr(x + y), env(x = 1, y = 1))

# Current environment:
eval(expr = expr(x + y))
```

One of the big advantages of evaluating code manually is that you can tweak the environment. There are two main reasons to do this:

- To temporarily override functions to implement a domain specific language.
- To add a data mask so you can refer to variables in a data frame as if they are variables in an environment.


## Customising evaluation with functions

The above example used an environment that bound `x` and `y` to vectors. It’s less obvious that **you also bind names to functions, allowing you to override the behaviour of existing functions. This is a big idea** that we’ll come back to in Chapter 21 where I explore generating HTML and LaTeX from R. The example below gives you a taste of the power. Here I evaluate code in a special environment where `*` and `+` have been overridden to work with strings instead of numbers:

```{r}
string_math <- function(x) {
  e <- env(
    caller_env(),
    
    `+` = function(x, y) {paste0(x, y)},
    `*` = function(x, y) {strrep(x, y)},
    `-` = function(x, y) {x + y}          # This is a trick (use + instead of -) !
  )
  
  eval(enexpr(x), e)
}

name <- "Yang"

string_math("Hi " + name)

string_math(("x" * 2 + "-y") * 3)

string_math(3 * 2)

string_math(2 - 3)   # This shows the result of the trick
```

dplyr takes this idea to the extreme, running code in an environment that generates SQL for execution in a remote database:

```{r}
library(dplyr)

con <- DBI::dbConnect(RSQLite::SQLite(), fillname = ":memory:")

mtcar_db <- copy_to(dest = con, df = mtcars)

mtcar_db |> 
  filter(cyl > 2) |> 
  select(mpg:hp) |> 
  head(10) |> 
  show_query()

DBI::dbDisconnect(con)
```


## Customising evaluation with data

**Rebinding functions is an extremely powerful technique, but it tends to require a lot of investment.** A more immediately practical application is modifying evaluation to look for variables in a data frame instead of an environment. 

This idea powers the base `subset()` and `transform()` functions, as well as many tidyverse functions like `ggplot2::aes()` and `dplyr::mutate()`.

It’s possible to use `eval()` for this, but there are a few potential pitfalls (Section 20.6), so we’ll switch to `rlang::eval_tidy()` instead.

As well as expression and environment, `eval_tidy()` also takes a **data mask**, which is typically a data frame:

```{r}
df <- data.frame(x = 1:5, y = 5:1)

df

eval_tidy(expr = expr(x + y), data = df)
```

Evaluating with a data mask is a useful technique for interactive analysis because it allows you to write `x + y` rather than `df$x + df$y`. However, that convenience comes at a cost: ambiguity. In Section 20.4 you’ll learn how to deal with ambiguity using special `.data` and `.env` pronouns.

Unfortunately, this function has a subtle bug and we need a new data structure to help deal with it.


## Quosures

To make the problem more obvious, I’m going to modify `with2()`. The basic problem still occurs without this modification but it’s much harder to see.


```{r}
with2 <- function(df, expr) {
  a <- 1000
  
  eval_tidy(enexpr(expr), df)
}
```

We can see the problem when we use `with2()` to refer to a variable called `a`. We want the value of `a` to come from the binding we can see (10), not the binding internal to the function (1000):

```{r}
df <- data.frame(x = 1:3)

a <- 10

with2(df, x + a)
```

The problem arises because we need to evaluate the captured expression in the environment where it was written (where `a` is 10), not the environment inside of `with2()` (where `a` is 1000).

Fortunately we can solve this problem by using a new data structure: the **quosure** which bundles an expression with an environment. `eval_tidy()` knows how to work with quosures so all we need to do is switch out `enexpr()` for `enquo()`:

```{r}
with2 <- function(df, expr) {
  a <- 1000
  
  eval_tidy(enquo(expr), df)
}

with2(df, x + a)
```

Whenever you use a data mask, you must always use `enquo()` instead of `enexpr()`. This is the topic of Chapter 20.
